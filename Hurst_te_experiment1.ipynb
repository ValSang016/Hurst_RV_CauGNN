{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T11:02:21.893959Z",
     "start_time": "2024-08-28T11:02:21.881955Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import datetime\n",
    "import os\n",
    "import time\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "import Optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator, MultipleLocator\n",
    "\n",
    "from hurst import compute_Hc\n",
    "\n",
    "from layer import DenseGraphConv\n",
    "\n",
    "\n",
    "np.seterr(divide='ignore',invalid='ignore')\n"
   ],
   "id": "2fc539aa980293d3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'warn', 'over': 'warn', 'under': 'ignore', 'invalid': 'warn'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-26T06:51:40.430995Z",
     "start_time": "2024-08-26T06:51:40.413994Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 각 값이 어떤 bin에 속하는지 인덱싱\n",
    "def value_to_bin(value_list, m, M, increments):\n",
    "    result = []\n",
    "    for v in value_list:\n",
    "        m_increments = m;\n",
    "        bin_increments = 0\n",
    "        while (m_increments < v):\n",
    "            m_increments = m_increments + increments\n",
    "            bin_increments = bin_increments + 1\n",
    "        result.append(bin_increments)\n",
    "    return result\n",
    "\n",
    "# calculate omega\n",
    "def error_cov_matrix(sigma, tau, P):\n",
    "    matrix = np.diag(np.diag(P.dot(tau * sigma).dot(P.T)))\n",
    "    return matrix\n",
    "\n",
    "\n",
    "inc = 0.01\n",
    "n_random = 25\n",
    "\n",
    "\n",
    "def TE_caculate(X, increments=inc):\n",
    "    \n",
    "    # 최대값, 최소값, bin size\n",
    "    M = np.max(X)\n",
    "    M_ceil = np.ceil(M / increments) * increments  # 반올림\n",
    "    m = np.min(X)\n",
    "    m_floor = np.floor(m / increments) * increments  # 반내림\n",
    "\n",
    "    # calculate TE\n",
    "    TE_matrix = np.zeros((X.shape[1], X.shape[1]))\n",
    "    for i in range(X.shape[1]):\n",
    "        for j in range(X.shape[1]):\n",
    "            if i == j:\n",
    "                continue\n",
    "            else:\n",
    "                # print(v, '->', w)\n",
    "                xn1 = [r[j] for r in X][1:]\n",
    "                xn = [r[j] for r in X][:-1]\n",
    "                yn = [r[i] for r in X][:-1]\n",
    "\n",
    "                # binning\n",
    "                xn1_bin = value_to_bin(xn1, m_floor, M_ceil, increments)\n",
    "                xn_bin = value_to_bin(xn, m_floor, M_ceil, increments)\n",
    "                yn_bin = value_to_bin(yn, m_floor, M_ceil, increments)\n",
    "\n",
    "                # bin freq\n",
    "                xn_freq = list(np.unique(np.array(xn_bin), return_counts=True))\n",
    "                xn_freq.append(xn_freq[1] / sum(xn_freq[1]))\n",
    "                xn_xn1_freq = list(np.unique(np.array([xn_bin, xn1_bin]).T, return_counts=True, axis=0))\n",
    "                xn_xn1_freq.append(xn_xn1_freq[1] / sum(xn_xn1_freq[1]))\n",
    "                xn_yn_freq = list(np.unique(np.array([xn_bin, yn_bin]).T, return_counts=True, axis=0))\n",
    "                xn_yn_freq.append(xn_yn_freq[1] / sum(xn_yn_freq[1]))\n",
    "                xn_xn1_yn_freq = list(np.unique(np.array([xn_bin, xn1_bin, yn_bin]).T, return_counts=True, axis=0))\n",
    "                xn_xn1_yn_freq.append(xn_xn1_yn_freq[1] / sum(xn_xn1_yn_freq[1]))\n",
    "\n",
    "                # calculate TE\n",
    "                TE_xy = 0\n",
    "                for ii, vv in enumerate(xn_xn1_yn_freq[0]):\n",
    "                    p_in_in1_jn = xn_xn1_yn_freq[2][ii]\n",
    "                    index_in_in1 = xn_xn1_freq[0].tolist().index(vv[:2].tolist())\n",
    "                    p_in_in1 = xn_xn1_freq[2][index_in_in1]\n",
    "                    index_in_jn = xn_yn_freq[0].tolist().index(vv[[0, 2]].tolist())\n",
    "                    p_in_jn = xn_yn_freq[2][index_in_jn]\n",
    "                    index_in = xn_freq[0].tolist().index(vv[0])\n",
    "                    p_in = xn_freq[2][index_in]\n",
    "                    TE_xy = TE_xy + p_in_in1_jn * np.log2(p_in_in1_jn * p_in / p_in_in1 / p_in_jn)\n",
    "                TE_matrix[i, j] = TE_xy\n",
    "\n",
    "    return TE_matrix\n",
    "\n",
    "\n",
    "def ETE_caculate(X, TE_matrix, increments=inc):\n",
    "    \n",
    "    # 최대값, 최소값, bin size\n",
    "    M = np.max(X);\n",
    "    M_ceil = np.ceil(M / increments) * increments  # 반올림\n",
    "    m = np.min(X);\n",
    "    m_floor = np.floor(m / increments) * increments  # 반내림\n",
    "    \n",
    "    # calculate RTE\n",
    "    RTE_matrix_all = np.zeros([X.shape[1], X.shape[1], n_random])\n",
    "    for nn in range(n_random):\n",
    "        # returns_shuffle = np.array(X).T.tolist()\n",
    "        # random.shuffle(returns_shuffle)\n",
    "        # returns_shuffle = np.array(returns_shuffle).T.tolist()\n",
    "        RTE_matrix = np.zeros((X.shape[1], X.shape[1]))\n",
    "        for i in range(X.shape[1]):\n",
    "            for j in range(X.shape[1]):\n",
    "                if i==j:\n",
    "                    continue\n",
    "                else:\n",
    "                    xn1 = [r[j] for r in X][1:]\n",
    "                    xn = [r[j] for r in X][:-1]\n",
    "                    yn = [r[i] for r in X][:-1]\n",
    "                    # binning\n",
    "                    xn1_bin = value_to_bin(xn1, m_floor, M_ceil, increments)\n",
    "                    xn_bin = value_to_bin(xn, m_floor, M_ceil, increments)\n",
    "                    yn_bin = value_to_bin(yn, m_floor, M_ceil, increments)\n",
    "                    np.random.shuffle(yn_bin)\n",
    "    \n",
    "                    # bin freq\n",
    "                    xn_freq = list(np.unique(np.array(xn_bin), return_counts=True))\n",
    "                    xn_freq.append(xn_freq[1]/sum(xn_freq[1]))\n",
    "                    xn_xn1_freq = list(np.unique(np.array([xn_bin, xn1_bin]).T, return_counts=True, axis=0))\n",
    "                    xn_xn1_freq.append(xn_xn1_freq[1]/sum(xn_xn1_freq[1]))\n",
    "                    xn_yn_freq = list(np.unique(np.array([xn_bin, yn_bin]).T, return_counts=True, axis=0))\n",
    "                    xn_yn_freq.append(xn_yn_freq[1]/sum(xn_yn_freq[1]))\n",
    "                    xn_xn1_yn_freq = list(np.unique(np.array([xn_bin, xn1_bin, yn_bin]).T, return_counts=True, axis=0))\n",
    "                    xn_xn1_yn_freq.append(xn_xn1_yn_freq[1]/sum(xn_xn1_yn_freq[1]))\n",
    "                    # calculate RTE\n",
    "                    RTE_xy = 0\n",
    "                    for ii, vv in enumerate(xn_xn1_yn_freq[0]):\n",
    "                        p_in_in1_jn = xn_xn1_yn_freq[2][ii]\n",
    "                        index_in_in1 = xn_xn1_freq[0].tolist().index(vv[:2].tolist())\n",
    "                        p_in_in1 = xn_xn1_freq[2][index_in_in1]\n",
    "                        index_in_jn = xn_yn_freq[0].tolist().index(vv[[0, 2]].tolist())\n",
    "                        p_in_jn = xn_yn_freq[2][index_in_jn]\n",
    "                        index_in = xn_freq[0].tolist().index(vv[0])\n",
    "                        p_in = xn_freq[2][index_in]\n",
    "                        RTE_xy = RTE_xy + p_in_in1_jn * np.log2(p_in_in1_jn * p_in / p_in_in1 / p_in_jn)\n",
    "                    RTE_matrix[i, j] = RTE_xy\n",
    "        RTE_matrix_all[:,:,nn] = RTE_matrix\n",
    "        # calculate ETE\n",
    "    \n",
    "    ETE_matrix = np.zeros((X.shape[1], X.shape[1]))\n",
    "    for i in range(X.shape[1]) :\n",
    "        for j in range(X.shape[1]):\n",
    "            if i == j:\n",
    "                continue\n",
    "            TE = TE_matrix[i, j]\n",
    "            rte_array = RTE_matrix_all[i, j, :]\n",
    "            if TE - np.mean(rte_array) - np.std(rte_array) / n_random ** 0.5 > 0:\n",
    "                ETE_matrix[i, j] = TE - np.mean(rte_array)\n",
    "    \n",
    "    return ETE_matrix"
   ],
   "id": "5fce0f61c178faac",
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-26T08:24:36.716105Z",
     "start_time": "2024-08-26T08:24:36.702104Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TENet(nn.Module):\n",
    "    def __init__(self, args, A, B, window_size):\n",
    "        super(TENet, self).__init__()\n",
    "        self.use_cuda = args.CUDA\n",
    "        self.batch_size = args.BATCH_SIZE\n",
    "        self.window_size = window_size\n",
    "\n",
    "        # A와 B 초기화\n",
    "        A = np.array(A, dtype=np.float32)\n",
    "        A = A / np.sum(A)\n",
    "        A_new = np.zeros((args.BATCH_SIZE, args.N_E, args.N_E), dtype=np.float32)\n",
    "        for i in range(args.BATCH_SIZE):\n",
    "            A_new[i, :, :] = A\n",
    "\n",
    "        self.A = torch.from_numpy(A_new).cuda() if self.use_cuda else torch.from_numpy(A_new)\n",
    "        self.adjs = [self.A]\n",
    "        self.num_adjs = args.NUM_ADJ\n",
    "        if self.num_adjs > 1:\n",
    "            A = np.array(B, dtype=np.float32)\n",
    "            A = A / np.sum(A, 1)\n",
    "            A_new = np.zeros((args.BATCH_SIZE, args.N_E, args.N_E), dtype=np.float32)\n",
    "            for i in range(args.BATCH_SIZE):\n",
    "                A_new[i, :, :] = A\n",
    "\n",
    "            self.B = torch.from_numpy(A_new).cuda() if self.use_cuda else torch.from_numpy(A_new)\n",
    "            self.C = torch.from_numpy(A_new).cuda()\n",
    "            self.adjs = [self.A,self.B,self.C]\n",
    "\n",
    "        self.n_e = args.N_E\n",
    "\n",
    "        # Conv2d 레이어\n",
    "        self.conv1=nn.Conv2d(1, args.CHANNEL_SIZE, kernel_size = (1,args.K_SIZE[0]),stride=1)\n",
    "        self.conv2=nn.Conv2d(1, args.CHANNEL_SIZE, kernel_size = (1,args.K_SIZE[1]),stride=1)\n",
    "        self.conv3=nn.Conv2d(1, args.CHANNEL_SIZE, kernel_size = (1,args.K_SIZE[2]),stride=1)\n",
    "\n",
    "\n",
    "        # Conv2d의 출력 차원 계산\n",
    "        d= (len(args.K_SIZE)*(self.window_size) -sum(args.K_SIZE)+ len(args.K_SIZE))*args.CHANNEL_SIZE\n",
    "\n",
    "        # GNN 레이어\n",
    "        self.gnn1 = DenseGraphConv(d, args.HID1)\n",
    "        self.gnn2 = DenseGraphConv(args.HID1, args.HID2)\n",
    "        self.gnn3 = DenseGraphConv(args.HID2, 1)\n",
    "\n",
    "        # Highway layer\n",
    "        self.hw = args.HIGHWAY_WINDOW\n",
    "        if self.hw > 0:\n",
    "            self.highway = nn.Linear(self.hw, 1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        c=x.permute(0,2,1)\n",
    "        c=c.unsqueeze(1)\n",
    "        # if self.decoder != 'GAT':\n",
    "        a1=self.conv1(c).permute(0,2,1,3).reshape(self.batch_size,self.n_e,-1)\n",
    "        a2=self.conv2(c).permute(0,2,1,3).reshape(self.batch_size,self.n_e,-1)\n",
    "        a3=self.conv3(c).permute(0,2,1,3).reshape(self.batch_size,self.n_e,-1)\n",
    "\n",
    "        x_conv = F.relu(torch.cat([a1, a2, a3], 2))\n",
    "        # x_conv=F.relu(torch.cat([a1,a2,a3,a4,a5],2))\n",
    "        x1 = F.relu(self.gnn1(x_conv,self.A))\n",
    "        x2 = F.relu(self.gnn2(x1,self.A))\n",
    "        x3 = self.gnn3(x2,self.A)\n",
    "        x3 = x3.squeeze()\n",
    "      \n",
    "\n",
    "        if self.hw>0:\n",
    "            z = x[:, -self.hw:, :]\n",
    "            z = z.permute(0, 2, 1)\n",
    "            z = self.highway(z)\n",
    "            z = z.squeeze(2)\n",
    "            x3 = x3 + z\n",
    "        return x3\n",
    "\n"
   ],
   "id": "8eb14fa0aa344437",
   "outputs": [],
   "execution_count": 77
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T09:38:42.635129Z",
     "start_time": "2024-08-28T09:38:42.603620Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "class WindowedDataset(Dataset):\n",
    "    def __init__(self, data, data_y, window_size, horizon, scaler_x, scaler_y, volatility_type):\n",
    "        self.data = data\n",
    "        self.data_y = data_y\n",
    "        self.window_size = window_size\n",
    "        self.horizon = horizon\n",
    "        self.scaler_x = scaler_x\n",
    "        self.scaler_y = scaler_y\n",
    "        self.volatility_type = volatility_type\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.window_size - self.horizon + 1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # 데이터 슬라이스 추출\n",
    "        window_data = self.data[idx:idx+self.window_size]\n",
    "        label = self.data_y[idx]\n",
    "\n",
    "        # 스케일링 (사전에 학습된 스케일러 사용)\n",
    "        window_scaled = self.scaler_x.transform(window_data)\n",
    "        label_scaled = self.scaler_y.transform(np.array([label]))\n",
    "        \n",
    "        return torch.tensor(window_scaled, dtype=torch.float32), torch.tensor(label_scaled.flatten(), dtype=torch.float32)\n",
    "\n",
    "\n",
    "class ModelManager:\n",
    "    def __init__(self, config, data, window_size, threshold, Effective, training_period):\n",
    "        self.config = config\n",
    "        self.window_size = window_size\n",
    "        self.raw_data = data\n",
    "        self.data_np = np.array(data.iloc[:, 1:])\n",
    "        self.data_y = self.calculate_realized_volatility(self.data_np, window_size)  # 실현 변동성 계산\n",
    "        self.training_period = training_period\n",
    "        self.threshold = threshold\n",
    "        self.Effective = Effective\n",
    "        self.device = torch.device(config.DEVICE)\n",
    "        self.model = None\n",
    "        self.criterion = nn.MSELoss().to(self.device)\n",
    "        self.optimizer = None\n",
    "        self.A = None\n",
    "        self.B = None\n",
    "        self.results = []\n",
    "        self.scaler_x = MinMaxScaler(feature_range=(0, 1))\n",
    "        self.scaler_y = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "        self.save_dir = None  # 저장할 디렉토리 경로\n",
    "\n",
    "        self.test_total_loss_list = []\n",
    "        self.hurst_values = []  # Hurst 지수를 저장하는 리스트\n",
    "        self.phase_changes = []  # 국면 변화가 발생한 시점을 저장하는 리스트\n",
    "        \n",
    "        # 각 주식별로 손실 값을 저장하기 위한 구조\n",
    "        self.test_each_loss_list = []\n",
    "\n",
    "        \n",
    "    def calculate_realized_volatility(self, data, window):\n",
    "        realized_vols = []\n",
    "        for i in range(window, len(data)):\n",
    "            window_data = data[i-window:i]  # 30일치 데이터 슬라이스\n",
    "            window_data = 100 * window_data\n",
    "            squared_returns = np.square(window_data)  # 제곱\n",
    "            realized_vol = np.sqrt(np.mean(squared_returns))  # 평균 계산 후 제곱근\n",
    "            realized_vols.append(realized_vol)\n",
    "        return np.array(realized_vols)\n",
    "\n",
    "    def load_model(self, A, B, window_size):\n",
    "        torch.cuda.empty_cache()\n",
    "        return TENet(A, B, window_size).to(self.device)  # GNN 모델 생성\n",
    "\n",
    "    def calculate_TE(self, data, Effective):\n",
    "        if Effective:\n",
    "            TE_matrix = TE_caculate(data)\n",
    "            ETE_matrix = ETE_caculate(data, TE_matrix)\n",
    "            return ETE_matrix\n",
    "        else:\n",
    "            TE_matrix = TE_caculate(data)\n",
    "            return TE_matrix\n",
    "\n",
    "    def update_A_B(self, matrix):\n",
    "        self.A = matrix\n",
    "        self.B = matrix\n",
    "\n",
    "    def calculate_hurst(self, data):\n",
    "        hurst_value = compute_Hc(data)[0]\n",
    "        return hurst_value\n",
    "\n",
    "    def check_phase_change(self, hurst_values, threshold=0.1):\n",
    "        # 국면 전환 여부를 결정하는 함수\n",
    "        if len(hurst_values) < 10:\n",
    "            return False\n",
    "        last_10 = hurst_values[-10:]\n",
    "        mean_h = np.mean(last_10)\n",
    "        std_h = np.std(last_10)\n",
    "        return std_h > threshold  # 단순히 표준편차가 일정 임계값을 초과하면 국면 전환으로 간주\n",
    "\n",
    "    def train_and_evaluate(self):\n",
    "        self.window_size = window_size\n",
    "\n",
    "        initial_data_x = self.data_np[:training_period]\n",
    "        initial_data_y = self.data_y[:training_period-window_size]\n",
    "        \n",
    "        te_matrix = self.calculate_TE(initial_data_x, self.Effective)\n",
    "        self.model = self.load_model(te_matrix, te_matrix, window_size)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.config.LR)\n",
    "        \n",
    "        self.train_model(initial_data_x, initial_data_y)\n",
    "    \n",
    "\n",
    "        for day in tqdm(range(training_period, len(self.data_np) - window_size + 1)):\n",
    "            start_idx = max(0, day - training_period)\n",
    "            current_data = self.data_np[start_idx:day]\n",
    "            \n",
    "            # Hurst 지수 계산\n",
    "            hurst_value = self.calculate_hurst(current_data)\n",
    "            self.hurst_values.append(hurst_value)\n",
    "            \n",
    "            # 과거 10일치 Hurst 지수를 이용한 국면 전환 체크\n",
    "            if len(self.hurst_values) >= 10:\n",
    "                last_10_days_h = self.hurst_values[-10:]\n",
    "                if self.check_phase_change(last_10_days_h):\n",
    "                    self.phase_changes.append(self.raw_data.index[day])\n",
    "\n",
    "                    # 새로운 국면에서 TE 재계산 및 GNN 재학습\n",
    "                    recent_3_year_data = self.data_np[day - training_period:day]\n",
    "                    te_matrix = self.calculate_TE(recent_3_year_data, self.Effective)\n",
    "                    self.model = self.load_model(te_matrix, te_matrix, window_size)\n",
    "                    self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.config.LR)\n",
    "                    self.train_model(recent_3_year_data)\n",
    "                \n",
    "            else:\n",
    "                print(f\"Insufficient data to check phase change on day {day}.\")\n",
    "\n",
    "            \n",
    "            TV_data_x = self.data_np[start_idx:day]\n",
    "            TV_data_y = self.data_y[start_idx-window_size:day-window_size]\n",
    "            \n",
    "            # 매일 실현 변동성 예측 및 손실 계산\n",
    "            test_data_x = self.data_x[day:day + window_size]\n",
    "            test_data_y = self.data_y[day:day + window_size]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # 매일 GNN 모델로 예측 및 손실 계산\n",
    "            self.train(train_loader, valid_loader)\n",
    "            test_total_loss, test_each_loss = self.evaluate_epoch(test_loader)\n",
    "            self.test_total_loss_list.append(test_total_loss)\n",
    "            self.test_each_loss_list.append(test_each_loss)\n",
    "\n",
    "            # 국면 전환 여부 확인\n",
    "            phase_changed = self.check_phase_change(self.hurst_values)\n",
    "            if phase_changed:\n",
    "                self.phase_changes.append(day)\n",
    "                print(f\"Phase change detected at index {day}. Reinitializing model.\")\n",
    "\n",
    "                # TE 재계산 및 모델 재학습\n",
    "                te_matrix = self.calculate_TE(train_data_x, self.Effective)\n",
    "                self.update_A_B(te_matrix)\n",
    "                self.model = self.load_model(self.A, self.B, window_size)\n",
    "                self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.config.LR)\n",
    "\n",
    "            else:\n",
    "                print(f\"No phase change detected on day {day}, continuing with existing model.\")\n",
    "\n",
    "\n",
    "\n",
    "    def train_model(self, x, y):\n",
    "        self.model.train()  # Set the model to training mode\n",
    "        \n",
    "        train_split = int(training_period * 0.7)\n",
    "        train_x = x[:train_split]\n",
    "        val_x = x[train_split:]\n",
    "        train_y = y[:train_split]\n",
    "        val_y = y[train_split:]\n",
    "        \n",
    "        self.scaler_x.fit(train_x)\n",
    "        self.scaler_y.fit(train_y)\n",
    "        \n",
    "        # 학습 데이터, 검증 데이터, 테스트 데이터 준비\n",
    "        train_loader = DataLoader(WindowedDataset(train_x, train_y, window_size, CONFIG.HORIZON, self.scaler_x, self.scaler_y), batch_size=self.config.BATCH_SIZE, shuffle=True,  drop_last=True)\n",
    "        val_loader = DataLoader(WindowedDataset(val_x, val_y, window_size, CONFIG.HORIZON, self.scaler_x, self.scaler_y), batch_size=self.config.BATCH_SIZE, shuffle=False,  drop_last=True)\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        best_model_state = None\n",
    "    \n",
    "        for epoch in range(CONFIG.EPOCHS):\n",
    "            train_total_loss = self.train_epoch(train_loader)\n",
    "            val_total_loss = self.evaluate_epoch(val_loader)            \n",
    "            # validation loss가 최저일 때 모델을 저장\n",
    "            if val_total_loss < best_val_loss:\n",
    "                best_val_loss = val_total_loss\n",
    "                best_model_state = self.model.state_dict()  # 모델의 가중치 저장\n",
    "        \n",
    "        self.model.load_state_dict(best_model_state)\n",
    "\n",
    "\n",
    "    def train_epoch(self, train_loader):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        each_loss = torch.zeros(self.data_x.shape[1]).to(self.device)  # 각 ETF에 대한 손실 초기화\n",
    "        n_samples = 0\n",
    "    \n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(self.device), y.to(self.device)\n",
    "            self.optimizer.zero_grad()\n",
    "            predictions = self.model(x)\n",
    "            loss = self.criterion(predictions, y)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "    \n",
    "            batch_size = x.size(0)\n",
    "            total_loss += loss.item() * batch_size  # 배치의 총 손실을 샘플 수로 가중치 더함\n",
    "            n_samples += batch_size\n",
    "    \n",
    "            # 각 ETF별 손실 합산\n",
    "            for i in range(self.data_x.shape[1]):\n",
    "                each_loss[i] += self.criterion(predictions[:, i:i + 1], y[:, i:i + 1]).item() * batch_size\n",
    "    \n",
    "        mse = each_loss / n_samples  # ETF별 평균 손실 계산\n",
    "        total_loss = mse.mean().item()  # 전체 평균 손실 계산\n",
    "    \n",
    "        return total_loss, mse.cpu().numpy()\n",
    "\n",
    "    def evaluate_epoch(self, loader):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        each_loss = torch.zeros(self.data_x.shape[1]).to(self.device)\n",
    "        n_samples = 0  # 전체 샘플 수\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            for x, y in loader:\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "                predictions = self.model(x)\n",
    "                loss = self.criterion(predictions, y)\n",
    "                \n",
    "                batch_size = x.size(0)\n",
    "                total_loss += loss.item() * batch_size  # 배치의 총 손실을 샘플 수로 가중치 더함\n",
    "                n_samples += batch_size\n",
    "    \n",
    "                # 각 ETF별 손실 합산\n",
    "                for i in range(self.data_x.shape[1]):\n",
    "                    each_loss[i] += self.criterion(predictions[:, i:i + 1], y[:, i:i + 1]).item() * batch_size\n",
    "    \n",
    "        mse = each_loss / n_samples  # ETF별 평균 손실 계산\n",
    "        total_loss = mse.mean().item()  # 전체 평균 손실 계산\n",
    "    \n",
    "        return total_loss, mse.cpu().numpy()\n",
    "    def create_save_directory(self):\n",
    "        # 현재 날짜와 시간을 기반으로 폴더 이름 생성\n",
    "        now = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        folder_name = f\"{self.Effective}_{self.volatility_type}_{self.window_size}_{self.training_period}_{self.testing_period}_{self.threshold}_{self.num_change_te}_{self.direction}_{now}\"\n",
    "        save_dir = os.path.join(os.getcwd(), folder_name)\n",
    "\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "\n",
    "        self.save_dir = save_dir\n",
    "        print(f\"Results will be saved in: {save_dir}\")\n",
    "\n",
    "    def plot_results(self):\n",
    "        if self.save_dir is None:\n",
    "            self.create_save_directory()\n",
    "    \n",
    "        # 전체 손실을 플롯\n",
    "        periods = range(1, len(self.train_total_loss_list) + 1)  # 1부터 시작하는 정수 시퀀스 생성\n",
    "        plt.figure(figsize=(14, 7))  # 그래프 크기를 키움\n",
    "        plt.plot(periods, self.train_total_loss_list, label='Train Loss')\n",
    "        plt.plot(periods, self.val_total_loss_list, label='Valid Loss')\n",
    "        plt.plot(periods, self.test_total_loss_list, label='Test Loss')\n",
    "        for idx in self.te_update_indices:\n",
    "            plt.axvline(x=idx, color='red', linestyle='--', label='TENet Update' if idx == self.te_update_indices[0] else \"\")\n",
    "        plt.title('Total Loss Over Time')\n",
    "        plt.xlabel('Period')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.xticks(ticks=range(1, len(self.train_total_loss_list) + 1, 5), rotation=45)\n",
    "        plt.grid(True)  # 눈금을 쉽게 볼 수 있도록 격자 표시\n",
    "        plt.savefig(os.path.join(self.save_dir, 'total_loss.png'))  # 그래프를 파일로 저장\n",
    "        # plt.show()\n",
    "    \n",
    "        # 주식별 손실을 플롯\n",
    "        num_stocks = len(self.train_each_loss_list[0])  # 첫 번째 에포크의 주식 수를 기준으로 주식의 수 결정\n",
    "    \n",
    "        for stock_idx in range(num_stocks):\n",
    "            plt.figure(figsize=(14, 7))  # 그래프 크기를 키움\n",
    "            train_losses = [epoch_loss[stock_idx] for epoch_loss in self.train_each_loss_list]\n",
    "            val_losses = [epoch_loss[stock_idx] for epoch_loss in self.val_each_loss_list]\n",
    "            test_losses = [epoch_loss[stock_idx] for epoch_loss in self.test_each_loss_list]\n",
    "    \n",
    "            plt.plot(periods, train_losses, label=f'ETF {stock_idx+1} Train Loss')\n",
    "            plt.plot(periods, val_losses, label=f'ETF {stock_idx+1} Valid Loss')\n",
    "            plt.plot(periods, test_losses, label=f'ETF {stock_idx+1} Test Loss')\n",
    "    \n",
    "            plt.title(f'ETF {stock_idx+1} Loss Over Time')\n",
    "            plt.xlabel('Period')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.legend()\n",
    "            plt.xticks(ticks=range(1, len(self.train_total_loss_list) + 1, 5), rotation=45)\n",
    "            plt.grid(True)  # 눈금을 쉽게 볼 수 있도록 격자 표시\n",
    "            plt.savefig(os.path.join(self.save_dir, f'ETF_{stock_idx+1}_loss.png'))  # 그래프를 파일로 저장\n",
    "            # plt.show()\n",
    "    \n",
    "        # TE 변화를 플롯\n",
    "        if self.TE_matrices:\n",
    "            # Outflow 그래프\n",
    "            plt.figure(figsize=(14, 7))  # 그래프 크기를 키움\n",
    "            for stock_idx in range(len(self.TE_matrices[0])):\n",
    "                outflow_te = [matrix[stock_idx, :].mean() for matrix in self.TE_matrices]\n",
    "                plt.plot(periods, outflow_te, label=f'ETF {stock_idx+1} Outflow TE')\n",
    "                \n",
    "            # TE 업데이트 지점에 빨간 선 추가\n",
    "            for idx in self.te_update_indices:\n",
    "                plt.axvline(x=idx, color='red', linestyle='--', label='TE or ETE Update' if idx == self.te_update_indices[0] else \"\")\n",
    "            plt.title('TE Changes Over Time (Outflow)')\n",
    "            plt.xlabel('Period')\n",
    "            plt.ylabel('TE Average')\n",
    "            plt.legend()\n",
    "            plt.xticks(ticks=range(1, len(self.train_total_loss_list) + 1, 5), rotation=45)\n",
    "            plt.grid(True)\n",
    "            plt.savefig(os.path.join(self.save_dir, 'te_outflow.png'))  # 그래프를 파일로 저장\n",
    "            # plt.show()\n",
    "    \n",
    "            # Inflow 그래프\n",
    "            plt.figure(figsize=(14, 7))  # 그래프 크기를 키움\n",
    "            for stock_idx in range(len(self.TE_matrices[0])):\n",
    "                inflow_te = [matrix[:, stock_idx].mean() for matrix in self.TE_matrices]\n",
    "                plt.plot(periods, inflow_te, label=f'ETF {stock_idx+1} Inflow TE')\n",
    "                \n",
    "            # TE 업데이트 지점에 빨간 선 추가\n",
    "            for idx in self.te_update_indices:\n",
    "                plt.axvline(x=idx, color='red', linestyle='--', label='TE or ETE Update' if idx == self.te_update_indices[0] else \"\")    \n",
    "            plt.title('TE Changes Over Time (Inflow)')\n",
    "            plt.xlabel('Period')\n",
    "            plt.ylabel('TE Average')\n",
    "            plt.legend()\n",
    "            plt.xticks(ticks=range(1, len(self.train_total_loss_list) + 1, 5), rotation=45)\n",
    "            plt.grid(True)\n",
    "            plt.savefig(os.path.join(self.save_dir, 'te_inflow.png'))  # 그래프를 파일로 저장\n",
    "            # plt.show()\n",
    "    \n",
    "            # 전체 평균 TE 변화 그래프\n",
    "            plt.figure(figsize=(14, 7))  # 그래프 크기를 키움\n",
    "            avg_te = [matrix.mean() for matrix in self.TE_matrices]\n",
    "            plt.plot(periods, avg_te, label='Overall TE Average')\n",
    "            for idx in self.te_update_indices:\n",
    "                plt.axvline(x=idx, color='red', linestyle='--', label='TE or ETE Update' if idx == self.te_update_indices[0] else \"\")\n",
    "            plt.title('Overall TE Changes Over Time')\n",
    "            plt.xlabel('Period')\n",
    "            plt.ylabel('TE Average')\n",
    "            plt.legend()\n",
    "            plt.xticks(ticks=range(1, len(self.train_total_loss_list) + 1, 5), rotation=45)\n",
    "            plt.grid(True)\n",
    "            plt.savefig(os.path.join(self.save_dir, 'te_overall.png'))  # 그래프를 파일로 저장\n",
    "            # plt.show()\n",
    "\n",
    "\n",
    "    def save_results_to_excel(self):\n",
    "        if self.save_dir is None:\n",
    "            self.create_save_directory()\n",
    "\n",
    "        filename = os.path.join(self.save_dir, f'Results_{self.threshold}_{self.num_change_te}_{self.direction}.xlsx')\n",
    "        with pd.ExcelWriter(filename) as writer:\n",
    "            # Losses\n",
    "            pd.DataFrame(self.train_total_loss_list, columns=['Train Loss']).to_excel(writer, sheet_name='Train Loss')\n",
    "            pd.DataFrame(self.val_total_loss_list, columns=['Validation Loss']).to_excel(writer, sheet_name='Validation Loss')\n",
    "            pd.DataFrame(self.test_total_loss_list, columns=['Test Loss']).to_excel(writer, sheet_name='Test Loss')\n",
    "\n",
    "            # ETF별 손실 저장\n",
    "            num_stocks = len(self.train_each_loss_list[0])  # ETF 개수 (첫 번째 기간에서의 ETF 수를 기준으로 결정)\n",
    "            periods = range(1, len(self.train_each_loss_list) + 1)  # 기간은 1부터 시작\n",
    "    \n",
    "            for stock_idx in range(num_stocks):\n",
    "                df = pd.DataFrame({\n",
    "                    'Period': periods,\n",
    "                    'Train Loss': [period_loss[stock_idx] for period_loss in self.train_each_loss_list],\n",
    "                    'Validation Loss': [period_loss[stock_idx] for period_loss in self.val_each_loss_list],\n",
    "                    'Test Loss': [period_loss[stock_idx] for period_loss in self.test_each_loss_list]\n",
    "                })\n",
    "                df.to_excel(writer, sheet_name=f'ETF {stock_idx+1} Loss', index=False)\n",
    "\n",
    "            # TE matrices\n",
    "            for i, matrix in enumerate(self.TE_matrices):\n",
    "                df = pd.DataFrame(matrix)\n",
    "                if self.Effective :\n",
    "                    df.to_excel(writer, sheet_name=f'Period {i+1} ETE')\n",
    "                else : \n",
    "                    df.to_excel(writer, sheet_name=f'Period {i+1} TE')\n",
    "                    \n",
    "            # TE Update Indices\n",
    "            if self.te_update_indices:\n",
    "                pd.DataFrame(self.te_update_indices, columns=['TE Update Indices']).to_excel(writer, sheet_name='TE Update Indices')\n",
    "\n",
    "    "
   ],
   "id": "713b8199e8b3da5",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T09:37:56.391408Z",
     "start_time": "2024-08-28T09:37:56.343895Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Config:\n",
    "    # Data parameters\n",
    "    N_E = 10\n",
    "    MODEL = 'TENet'\n",
    "    K_SIZE = [3, 5, 7]\n",
    "    DECODER = 'GNN'\n",
    "    HORIZON = 1\n",
    "    NUM_ADJ = 1\n",
    "    HIGHWAY_WINDOW = 0\n",
    "    \n",
    "    EPOCHS = 100\n",
    "    BATCH_SIZE = 32\n",
    "    \n",
    "    \n",
    "    SKIP_MODE = \"none\"\n",
    "    ATTENTION_MODE = \"naive\"\n",
    "    CHANNEL_SIZE = 12\n",
    "    HID1 = 20\n",
    "    HID2 = 30\n",
    "    HIDRNN = 100\n",
    "    RNN_LAYERS = 1\n",
    "    HIDCNN = 12\n",
    "    CNN_KERNEL = 6\n",
    "    N_HEAD = 8\n",
    "    D_K = 64\n",
    "    D_V = 64\n",
    "    SEED = 54321\n",
    "    \n",
    "    GPU = 0\n",
    "    \n",
    "    LOG_INTERVAL = 2000\n",
    "    SAVE = 'model/model.pt'\n",
    "    CUDA = True\n",
    "    OPTIM = 'adam'\n",
    "    LR = 0.001\n",
    "    L1LOSS = False\n",
    "    SKIP = 24\n",
    "    HIDSKIP = 10\n",
    "    NORMALIZE = 1\n",
    "    OUTPUT_FUN = 'Linear'\n",
    "    \n",
    "    DEVICE = 'cuda:0'\n",
    "    \n",
    "    GCN_TRUE = True\n",
    "    BUILDA_TRUE = True\n",
    "    GCN_DEPTH = 2\n",
    "    NUM_NODES = 8\n",
    "    DROPOUT = 0.2\n",
    "    SUBGRAPH_SIZE = 4\n",
    "    NODE_DIM = 40\n",
    "    DILATION_EXPONENTIAL = 2\n",
    "    CONV_CHANNELS = 12\n",
    "    RESIDUAL_CHANNELS = 12\n",
    "    SKIP_CHANNELS = 32\n",
    "    END_CHANNELS = 64\n",
    "    IN_DIM = 1\n",
    "    SEQ_IN_LEN = 32\n",
    "    SEQ_OUT_LEN = 1\n",
    "    LAYERS = 5\n",
    "    WEIGHT_DECAY = 0.00001\n",
    "    CLIP = 10\n",
    "    PROPALPHA = 0.05\n",
    "    TANHALPHA = 3\n",
    "    NUM_SPLIT = 1\n",
    "    STEP_SIZE = 100\n",
    "    VALIDATION_FREQ=10\n",
    "\n",
    "CONFIG = Config()\n",
    "\n",
    "\n",
    "CONFIG.CUDA = torch.cuda.is_available()\n",
    "if CONFIG.CUDA:\n",
    "    torch.cuda.set_device(CONFIG.GPU)"
   ],
   "id": "de286cef6f7b930",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T10:45:48.688552Z",
     "start_time": "2024-08-28T10:45:48.681045Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "6c3055a4077b40f5",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "logR_df = pd.read_csv('data/log_df_etf10.csv')\n",
    "\n",
    "manager = ModelManager(CONFIG, logR_df, window_size=30, training_period=250, threshold=None, Effective=False)\n",
    "\n",
    "# 저장할 디렉토리 설정\n",
    "manager.save_dir = \"/\"\n",
    "                    \n",
    "# 학습 및 평가\n",
    "manager.train_and_evaluate()  # Example sizes in days\n",
    "                    \n",
    "# 결과 플롯 및 저장\n",
    "manager.plot_results()\n",
    "manager.save_results_to_excel()\n",
    "                    "
   ],
   "id": "d34aeff0d00294f4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-26T08:37:00.183597Z",
     "start_time": "2024-08-26T08:36:44.225263Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Configuration and initialization\n",
    "fin = open(\"/Users/sangheon/Desktop/CauGNN-main/TENet-master/data/log_df_etf10.txt\")\n",
    "data = np.loadtxt(fin, delimiter=',')\n",
    "\n",
    "# 실험을 위한 파라미터 값 설정\n",
    "window_sizes = [30, 60, 90]\n",
    "training_periods = [750, 1500]\n",
    "# num_change_tes = [4, 8]\n",
    "# directions = ['outflow', 'inflow']\n",
    "effectives = [False, True]\n",
    "\n",
    "# For문을 통해 모든 조합에 대해 실험 수행\n",
    "for window_size in tqdm(window_sizes, leave=False):\n",
    "    for training_period in tqdm(training_periods, leave=False):\n",
    "                for effective in tqdm(effectives, leave=False):\n",
    "                    # 각 조합에 대해 디렉토리 생성\n",
    "                    experiment_name = f\"Window_{window_size}_Train_{training_period}_Effective_{effective}\"\n",
    "                    save_dir = os.path.join(\"Realized_experiments_change_all_period\", experiment_name)\n",
    "                    os.makedirs(save_dir, exist_ok=True)\n",
    "                    \n",
    "                    # ModelManager 객체 생성\n",
    "                    manager = ModelManager(CONFIG, data, threshold=None, Effective=effective, volatility_type=2, num_change_te=None, direction=None)\n",
    "                    \n",
    "                    # 저장할 디렉토리 설정\n",
    "                    manager.save_dir = save_dir\n",
    "                    \n",
    "                    # 학습 및 평가\n",
    "                    manager.train_and_evaluate(window_size=window_size, training_period=250, testing_period=60)  # Example sizes in days\n",
    "                    \n",
    "                    # 결과 플롯 및 저장\n",
    "                    manager.plot_results()\n",
    "                    manager.save_results_to_excel()\n"
   ],
   "id": "c7d824187d5386a7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "  0%|          | 0/85 [00:00<?, ?it/s]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "  1%|          | 1/85 [00:04<05:44,  4.10s/it]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "  2%|▏         | 2/85 [00:07<05:26,  3.94s/it]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "  4%|▎         | 3/85 [00:11<05:21,  3.92s/it]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "  5%|▍         | 4/85 [00:15<05:21,  3.97s/it]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "                                     \u001B[A\u001B[A\n",
      "                                     \u001B[A\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[112], line 31\u001B[0m\n\u001B[0;32m     28\u001B[0m manager\u001B[38;5;241m.\u001B[39msave_dir \u001B[38;5;241m=\u001B[39m save_dir\n\u001B[0;32m     30\u001B[0m \u001B[38;5;66;03m# 학습 및 평가\u001B[39;00m\n\u001B[1;32m---> 31\u001B[0m \u001B[43mmanager\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_and_evaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwindow_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwindow_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtraining_period\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m250\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtesting_period\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m60\u001B[39;49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Example sizes in days\u001B[39;00m\n\u001B[0;32m     33\u001B[0m \u001B[38;5;66;03m# 결과 플롯 및 저장\u001B[39;00m\n\u001B[0;32m     34\u001B[0m manager\u001B[38;5;241m.\u001B[39mplot_results()\n",
      "Cell \u001B[1;32mIn[107], line 192\u001B[0m, in \u001B[0;36mModelManager.train_and_evaluate\u001B[1;34m(self, window_size, training_period, testing_period)\u001B[0m\n\u001B[0;32m    188\u001B[0m test_loader \u001B[38;5;241m=\u001B[39m DataLoader(WindowedDataset(test_data, test_data_y, window_size, CONFIG\u001B[38;5;241m.\u001B[39mHORIZON, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscaler_x, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscaler_y, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvolatility_type), batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mBATCH_SIZE, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,  drop_last\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m    191\u001B[0m \u001B[38;5;66;03m# TE 초기 계산과 재계산 및 모델 설정\u001B[39;00m\n\u001B[1;32m--> 192\u001B[0m TE, ETE \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcalculate_TE\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mEffective\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    193\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mEffective :\n\u001B[0;32m    194\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mTE_matrices\u001B[38;5;241m.\u001B[39mappend(ETE)\n",
      "Cell \u001B[1;32mIn[107], line 76\u001B[0m, in \u001B[0;36mModelManager.calculate_TE\u001B[1;34m(self, data, Effective)\u001B[0m\n\u001B[0;32m     74\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m TE_matrix, ETE_matrix\n\u001B[0;32m     75\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 76\u001B[0m     TE_matrix \u001B[38;5;241m=\u001B[39m \u001B[43mTE_caculate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minc\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     77\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m TE_matrix, \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[56], line 49\u001B[0m, in \u001B[0;36mTE_caculate\u001B[1;34m(X, increments)\u001B[0m\n\u001B[0;32m     46\u001B[0m yn_bin \u001B[38;5;241m=\u001B[39m value_to_bin(yn, m_floor, M_ceil, increments)\n\u001B[0;32m     48\u001B[0m \u001B[38;5;66;03m# bin freq\u001B[39;00m\n\u001B[1;32m---> 49\u001B[0m xn_freq \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munique\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43marray\u001B[49m\u001B[43m(\u001B[49m\u001B[43mxn_bin\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_counts\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m)\n\u001B[0;32m     50\u001B[0m xn_freq\u001B[38;5;241m.\u001B[39mappend(xn_freq[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m/\u001B[39m \u001B[38;5;28msum\u001B[39m(xn_freq[\u001B[38;5;241m1\u001B[39m]))\n\u001B[0;32m     51\u001B[0m xn_xn1_freq \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(np\u001B[38;5;241m.\u001B[39munique(np\u001B[38;5;241m.\u001B[39marray([xn_bin, xn1_bin])\u001B[38;5;241m.\u001B[39mT, return_counts\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m))\n",
      "File \u001B[1;32m<__array_function__ internals>:200\u001B[0m, in \u001B[0;36munique\u001B[1;34m(*args, **kwargs)\u001B[0m\n",
      "File \u001B[1;32m~\\Desktop\\CauGNN-main\\venv\\lib\\site-packages\\numpy\\lib\\arraysetops.py:274\u001B[0m, in \u001B[0;36munique\u001B[1;34m(ar, return_index, return_inverse, return_counts, axis, equal_nan)\u001B[0m\n\u001B[0;32m    272\u001B[0m ar \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39masanyarray(ar)\n\u001B[0;32m    273\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m axis \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 274\u001B[0m     ret \u001B[38;5;241m=\u001B[39m \u001B[43m_unique1d\u001B[49m\u001B[43m(\u001B[49m\u001B[43mar\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_inverse\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_counts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[0;32m    275\u001B[0m \u001B[43m                    \u001B[49m\u001B[43mequal_nan\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mequal_nan\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    276\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _unpack_tuple(ret)\n\u001B[0;32m    278\u001B[0m \u001B[38;5;66;03m# axis was specified and not None\u001B[39;00m\n",
      "File \u001B[1;32m~\\Desktop\\CauGNN-main\\venv\\lib\\site-packages\\numpy\\lib\\arraysetops.py:336\u001B[0m, in \u001B[0;36m_unique1d\u001B[1;34m(ar, return_index, return_inverse, return_counts, equal_nan)\u001B[0m\n\u001B[0;32m    334\u001B[0m     aux \u001B[38;5;241m=\u001B[39m ar[perm]\n\u001B[0;32m    335\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 336\u001B[0m     \u001B[43mar\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msort\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    337\u001B[0m     aux \u001B[38;5;241m=\u001B[39m ar\n\u001B[0;32m    338\u001B[0m mask \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mempty(aux\u001B[38;5;241m.\u001B[39mshape, dtype\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mbool_)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 112
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-26T08:36:42.629349Z",
     "start_time": "2024-08-26T08:36:42.629349Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Configuration and initialization\n",
    "fin = open(\"/Users/sangheon/Desktop/CauGNN-main/TENet-master/data/log_df_etf10.txt\")\n",
    "data = np.loadtxt(fin, delimiter=',')\n",
    "manager = ModelManager(CONFIG, data, threshold=None, Effective=True, volatility_type=1, num_change_te=None, direction=None)\n",
    "manager.train_and_evaluate(window_size=32, training_period=750, testing_period=250)  # Example sizes in days\n",
    "manager.plot_results()\n",
    "manager.save_results_to_excel()"
   ],
   "id": "588c5d23d5fac389",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T05:07:27.568380Z",
     "start_time": "2024-08-14T05:07:27.568380Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    "변수\n",
    "training period size : big window size of train\n",
    "testing period size : big window size of test & big window moving size\n",
    "window_size : small window moving size\n",
    "threshold : TE or ETE threshold\n",
    "volatility type : , 0 for log return, 1 for absolute log return, 2 for realized volatility\n",
    "\n",
    "horizon : 1 (small window moving size) / Fixed\n",
    "training rate : 0.7 in train set(on training period size) & Shuffled / Fixed\n",
    "validation rate : 0.3 in train set / Fixed\n",
    "\n",
    "changing point index(TE 임계값 넘는 지점)\n",
    "changing point list\n",
    "TE list\n",
    "ETE list\n",
    "Loss list of each stock\n",
    "Loss list of all\n",
    "\n",
    "\n",
    "특정 Threshold에 따라 모델 변경\n",
    "전체 ETF의 TE 평균, inflow TE의 평균, outflow TE의 평균, inflow의 평균이 threshold를 넘는 ETF의 개수, outflow의 평균이 threshold를 넘는 ETF의 개수\n",
    "총 5개의 경우\n",
    "\n",
    "공통\n",
    "window size : 3m, 6m, 1y, 3y, 5y\n",
    "'''"
   ],
   "id": "5835808af4d0b990",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
